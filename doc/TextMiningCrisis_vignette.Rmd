---
title: "Introduction to TextMiningCrisis: <br> an accessible framework for Natural Language Processing"
author: "Manuel Betin, Umberto Collodel"
date: "3/20/2020"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to TextMiningCrisis: an accessible framework for Natural Language Processing}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(eval = F)
```

Getting started with text analysis can be a daunting task for researchers/practioners. <br> There are different elements of confusion: mainly, the many steps to take before the selected documents can be actually analyzed as quantitative data and the variety of functions coming from different packages, each with its own trade-offs. <br> 
The purpose of the TextMiningCrisis package is to make Natural Language Processing tasks easier, hence more accessible and reproducible, consolidating the different steps into an unified framework.


![Stylized representation of normal steps into a NPL task](TextMiningCrisis_process.png)

There are five families of functions in the package to help you thorugh the different stages:
<ol>
  <li>Lexicon: these functions allow to create and inspect your own dictionary of expressions related to the event of interest.</li>
  <li>Text extraction: tools to download files and read (aggregate) into the global environment. </li>
  <li>Text cleaning: cleanse corpus from elements that would hinder Natural Language Processing.</li>
  <li>Term frequencies: set of functions to compute tf-idfs from the corpus.</li>
  <li> Comparison and refine: check eventual mistakes and render updating easy.
</ol>

#Selection
You can create your own dictionary of expressions for multiple categories.<br>
In this case, we show the keywords we created for natural disasters and conflicts:
```{r warning=FALSE}
TextMiningCrisis::lexicon() [c("Natural_disaster","Wars")]
```

#Pre-processing
##Download files
The function ```pdf_from_url``` downloads all the pdfs from a dataframe of urls into a specified directory.
Three messages can appear during the process of download:

1. name_file: succesfully downloaded
1. name_file: already downloaded
1. name_file: Error in path file




##Aggregate corpus
The function ```aggregate_corpus``` loads all the files present into a directory into a single list and cleans them.

```{r warning=FALSE,results='hide'}
## Aggregate files in docs_example directory (IMF documents for Argentina 1980-1990):

#corpus <- TextMiningCrisis::aggregate_corpus("../data/docs_example", only_files = T)
```

```{r}
# Show the second page of the first document:
data("IMF_corpus_example")
corpus=IMF_corpus_example
corpus[[1]][[2]]
```



#NPL
```run_tf``` returns from the corpus of documents a term-frequency matrix. What is it? For each of the categories in the ```lexicon``` function, counts the occurence of the selected keywords in every document and divides it by the total number of characters.<br> <br>
In this example, we want to quantify the use of words related to currency crisis and wars in documents on Argentina published by the IMF. The period our corpus covers is 1970-1980:
```{r warning=FALSE,results='hide'}


# Create a term-frequency matrix for previous corpus:
data("IMF_tf_example")

tf <-TextMiningCrisis::run_tf("../data/corpus_example.RData",keyword_list = c("Currency_crisis_severe","Wars"))

```

```{r, echo=F}
# Display matrix:
tf
```
Let's plot the two indexes:


```{r, include=FALSE}
# Load packages used in this session:

library(ggplot2)
library(tidyverse)
```

```{r fig.width=7}

# Extract year and iso3, average over documents by year:

tf_year <- tf %>% 
  mutate(year = str_extract(file,"\\d{4}")) %>%
  mutate(iso3c = str_extract(file,"[A-Z]{3}")) %>%
  group_by(iso3c,year) %>% 
  summarize_at(vars(c("Currency_crisis_severe","Wars")), mean, na.rm = TRUE) %>% 
  gather("tf_type","tf_value",Currency_crisis_severe:Wars)

# Plot:

tf_year %>% 
  ggplot(aes(year,tf_value*100,col=tf_type,group = 1)) +
  geom_line() +
  facet_wrap(~tf_type) +
  theme_bw() +
  ylab("%") +
  xlab("") +
  theme(legend.position = "none") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) 


```

There seems to be three different waves of currency crises, in 1981,1983-1985 and 1989. In the last year of the decade the tf reaches its maximumum: 0.01\% of the documents' characters is on average devoted to currency crisis. For wars, instead, the index starts rising in 1983 and reaches its peak in 1985.


##Under the hood: important add-ons

<ol>
  <li> Avoid problematic documents:<br>
  In the scraping process, we might end up with some URLs of problematic documents i.e. documents on which we do not want to conduct the tf extraction. In our example, we downloaded many working paper together with. If this kind of information is not present into the metadata, the next logical step to take would be the manually erase these documents: this, in turn, would be tedious and take a long time. <br> <br>
  We incorporate an automated alternative to the best of our possibilities in the ```run_tf``` function. Given a set of keywords in our dictionary (included in the category Problematic_documents)
  
```{r}
TextMiningCrisis::lexicon()[c("Problematic_documents")] %>% data.frame() %>% filter(str_detect(Problematic_documents, "working"))
```
  
  
  the function checks if at least one of them appears in the first page of each document of the corpus: if the logical returns TRUE, the extraction is not performed and an NA is returned in the dataframe for the respective file.
  
  
  <li> Confusing keywords: sometimes words included in a category may refer to another individual e.g. talking about currency   crisis in other countries. <br> <br>
  We offer the possibility to control for this issue for any category of interest in the lexicon. It is sufficient to create a   category with same name, ending with "*_confusing". Here an example for currency crises:
```{r}
TextMiningCrisis::lexicon()[c("Currency_crisis_confusing")] %>% data.frame() 
```

The correction clearly will not run for the countries/individuals the confusing keywords refer to.

</ol>


#Comparison
Now, you are a bit surprised by the result, you did not know about any currency crisis for Argentina in 1989. Nevertheless, there it is the peak of your index. <br>
Is it a colossal blunder or indeed the truth?
What are the exact keywords that were detected? and in which context? <br>
To check, we can use the ```get_sentences``` function. 

```{r}

sentences_currency_crisis <- TextMiningCrisis::get_sentences(corpus, c("Currency_crisis_severe"))
sentences_currency_crisis[["ARG_1989-10-17_request"]]

```
Was the IMF really talking about currency crises in that year for Argentina or the lexicon we used has some issues that need to be addressed?<br>

```{r}
# Extract the third sentence with keyword detected in a randomly chosen 1989 document:

sentences_currency_crisis[["ARG_1989-10-17_request"]] %>% 
  select(sentence) %>% 
  slice(3) %>% 
  data.frame()

```
It is now clear that it was not a mistake and we even have additional information about the timing of the shock.

##Need to re-run
Assume there is a mistake in your extraction and you want to repeat it for a single category, that you want to compare your index adding or removing a keyword or that you simply want to add an index for a new category without changing the others. This can easily achieved with the ```run_tf_update``` function:

```{r echo=T,results='hide'}
# In this example we add an index for natural disasters events to the previous extraction:

tf_path = "../data/tf_crisis_words_example.RData"
corpus_path = "../data/corpus_example.RData"

updated_tf <-TextMiningCrisis::run_tf_update(path_tf_to_update = tf_path, corpus_path = corpus_path, keyword_list = c("Natural_disaster"))
```
If we average the index by year and plot as in the previous example:
```{r echo=F,fig.width=7}

# Average by year and plot as before:

updated_tf_year <- updated_tf %>% 
  mutate(year = str_extract(file,"\\d{4}")) %>%
  mutate(iso3c = str_extract(file,"[A-Z]{3}")) %>%
  group_by(iso3c,year) %>% 
  summarize(Natural_disaster = mean(Natural_disaster, na.rm = TRUE))

updated_tf_year %>% 
  ggplot(aes(year,Natural_disaster*100,col = iso3c, group = 1)) +
  geom_line() +
  theme_bw() +
  ylab("%") +
  xlab("") +
  theme(legend.position = "none") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) 

TextMiningCrisis::get_sentences(corpus, c("Natural_disaster"))[["ARG_1988-02-26_Use fund"]] %>% select(sentence) %>% slice(10) %>% data.frame()



```
The index seems rather volatile for Argentina throughout the period, but there is a marked rising in 1985 that ends with a peak in 1988. Indeed, the function ```get_sentence``` confirms the result of the index.

#A function to conquer them all:

You understood the purpose of all the main functions, but you would like to summarise the whole in just a line of code and you are still not confortable to write your own function? No problem, we thought about you and came up with the ```run_tf_by_chunk``` function.



